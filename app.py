import streamlit as st
import requests
import json
import time
from typing import List, Dict, Any
import logging
import PyPDF2
import docx
import io
import re
from datetime import datetime
from urllib.parse import urlparse
import plotly.graph_objects as go
import plotly.express as px
from streamlit_option_menu import option_menu
import openai

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
API_TOKEN = "9a653094793aedeae46f194aa755e2bb17f297f5209b7f99c1ced3671779d95d"
OPENAI_API_KEY = "your-openai-api-key"  # You'll need to set this

class DocumentProcessor:
    """Enhanced document processor with better error handling"""
    
    def __init__(self):
        self.max_file_size = 50 * 1024 * 1024  # 50MB
    
    def process_document(self, document_url: str) -> Dict[str, Any]:
        """Process document from URL with progress tracking"""
        try:
            # Download document
            response = requests.get(document_url, timeout=60, stream=True)
            if response.status_code != 200:
                raise Exception(f"Failed to download: HTTP {response.status_code}")
            
            content_type = response.headers.get('content-type', '').lower()
            content = response.content
            
            # Extract content based on type
            if 'pdf' in content_type or document_url.lower().endswith('.pdf'):
                return self._extract_pdf_content(content)
            elif 'word' in content_type or document_url.lower().endswith(('.docx', '.doc')):
                return self._extract_docx_content(content)
            else:
                return self._extract_text_content(content)
                
        except Exception as e:
            logger.error(f"Error processing document: {str(e)}")
            raise Exception(f"Failed to process document: {str(e)}")
    
    def _extract_pdf_content(self, content: bytes) -> Dict[str, Any]:
        """Extract content from PDF with enhanced processing"""
        try:
            pdf_file = io.BytesIO(content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text_parts = []
            page_info = []
            
            for i, page in enumerate(pdf_reader.pages):
                try:
                    text = page.extract_text()
                    if text and text.strip():
                        cleaned_text = self._clean_text(text)
                        text_parts.append(cleaned_text)
                        page_info.append({
                            'page': i + 1,
                            'text_length': len(cleaned_text),
                            'word_count': len(cleaned_text.split())
                        })
                except Exception:
                    continue
            
            full_text = ' '.join(text_parts)
            chunks = self._create_smart_chunks(full_text)
            
            return {
                'full_text': full_text,
                'chunks': chunks,
                'page_count': len(pdf_reader.pages),
                'pages_with_content': len(page_info),
                'page_info': page_info,
                'content_type': 'PDF',
                'word_count': len(full_text.split()),
                'char_count': len(full_text),
                'success': True
            }
            
        except Exception as e:
            return {
                'full_text': '',
                'chunks': [],
                'page_count': 0,
                'content_type': 'PDF',
                'success': False,
                'error': str(e)
            }
    
    def _extract_docx_content(self, content: bytes) -> Dict[str, Any]:
        """Extract content from DOCX with enhanced processing"""
        try:
            doc_file = io.BytesIO(content)
            doc = docx.Document(doc_file)
            
            text_parts = []
            paragraph_info = []
            
            for i, paragraph in enumerate(doc.paragraphs):
                if paragraph.text and paragraph.text.strip():
                    cleaned_text = self._clean_text(paragraph.text)
                    text_parts.append(cleaned_text)
                    paragraph_info.append({
                        'paragraph': i + 1,
                        'text_length': len(cleaned_text),
                        'word_count': len(cleaned_text.split())
                    })
            
            full_text = ' '.join(text_parts)
            chunks = self._create_smart_chunks(full_text)
            
            return {
                'full_text': full_text,
                'chunks': chunks,
                'page_count': len(doc.paragraphs),
                'paragraphs_with_content': len(paragraph_info),
                'paragraph_info': paragraph_info,
                'content_type': 'DOCX',
                'word_count': len(full_text.split()),
                'char_count': len(full_text),
                'success': True
            }
            
        except Exception as e:
            return {
                'full_text': '',
                'chunks': [],
                'page_count': 0,
                'content_type': 'DOCX',
                'success': False,
                'error': str(e)
            }
    
    def _extract_text_content(self, content: bytes) -> Dict[str, Any]:
        """Extract content from plain text"""
        try:
            text = content.decode('utf-8', errors='ignore')
            cleaned_text = self._clean_text(text)
            chunks = self._create_smart_chunks(cleaned_text)
            
            return {
                'full_text': cleaned_text,
                'chunks': chunks,
                'page_count': 1,
                'content_type': 'TEXT',
                'word_count': len(cleaned_text.split()),
                'char_count': len(cleaned_text),
                'success': True
            }
            
        except Exception as e:
            return {
                'full_text': '',
                'chunks': [],
                'page_count': 0,
                'content_type': 'TEXT',
                'success': False,
                'error': str(e)
            }
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep important punctuation
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-$$$$\[\]\"\'\/\%\$]', '', text)
        return text.strip()
    
    def _create_smart_chunks(self, text: str, chunk_size: int = 1200, overlap: int = 200) -> List[str]:
        """Create intelligent text chunks with overlap"""
        if not text or not text.strip():
            return []
        
        # Split by sentences
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # Check if adding this sentence would exceed chunk size
            if len(current_chunk) + len(sentence) + 2 <= chunk_size:
                current_chunk += sentence + ". "
            else:
                # Save current chunk
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                
                # Start new chunk with overlap
                if chunks and overlap > 0:
                    # Add last few sentences from previous chunk
                    prev_sentences = chunks[-1].split('. ')
                    overlap_sentences = prev_sentences[-2:] if len(prev_sentences) > 2 else prev_sentences
                    current_chunk = '. '.join(overlap_sentences) + ". " + sentence + ". "
                else:
                    current_chunk = sentence + ". "
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk) > 100]

class GPTQueryEngine:
    """GPT-4 powered query engine"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or OPENAI_API_KEY
        if self.api_key and self.api_key != "your-openai-api-key":
            openai.api_key = self.api_key
            self.gpt_available = True
        else:
            self.gpt_available = False
    
    def find_relevant_content(self, question: str, chunks: List[str], top_k: int = 3) -> List[Dict[str, Any]]:
        """Find relevant content using keyword matching and scoring"""
        question_lower = question.lower()
        question_words = set([word for word in question_lower.split() if len(word) > 3])
        
        scored_chunks = []
        for i, chunk in enumerate(chunks):
            chunk_lower = chunk.lower()
            chunk_words = set(chunk_lower.split())
            
            # Calculate relevance score
            word_overlap = len(question_words.intersection(chunk_words))
            total_words = len(question_words.union(chunk_words))
            
            if total_words > 0:
                relevance_score = word_overlap / len(question_words)
                
                # Boost score for exact phrase matches
                for word in question_words:
                    if word in chunk_lower:
                        relevance_score += 0.1
                
                if relevance_score > 0:
                    scored_chunks.append({
                        'chunk': chunk,
                        'score': relevance_score,
                        'index': i,
                        'word_overlap': word_overlap
                    })
        
        # Sort by score and return top chunks
        scored_chunks.sort(key=lambda x: x['score'], reverse=True)
        return scored_chunks[:top_k]
    
    def generate_answer_with_gpt(self, question: str, context: str) -> str:
        """Generate answer using GPT-4"""
        if not self.gpt_available:
            return self._generate_fallback_answer(question, context)
        
        try:
            prompt = f"""You are an expert document analyst. Based on the provided context, answer the question accurately and concisely.

Context: {context[:3000]}

Question: {question}

Instructions:
- Provide a direct, accurate answer based only on the context
- If the information is not in the context, state this clearly
- For insurance documents, focus on specific terms, conditions, and coverage details
- Include relevant details like time periods, amounts, or conditions when available

Answer:"""

            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a helpful document analyst specializing in insurance, legal, and compliance documents."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"GPT-4 API error: {str(e)}")
            return self._generate_fallback_answer(question, context)
    
    def _generate_fallback_answer(self, question: str, context: str) -> str:
        """Generate fallback answer using pattern matching"""
        question_lower = question.lower()
        context_lower = context.lower()
        
        # Insurance-specific patterns
        if 'grace period' in question_lower:
            grace_match = re.search(r'grace period of (\w+(?:-\w+)?)\s*days?', context_lower)
            if grace_match:
                return f"A grace period of {grace_match.group(1)} days is provided for premium payment."
        
        elif 'waiting period' in question_lower:
            waiting_match = re.search(r'waiting period of (\w+(?:-\w+)?)\s*(?:$$\d+$$)?\s*(months?|years?)', context_lower)
            if waiting_match:
                period, unit = waiting_match.groups()
                return f"There is a waiting period of {period} {unit} for this coverage."
        
        elif 'maternity' in question_lower and 'cover' in question_lower:
            if 'maternity' in context_lower and any(word in context_lower for word in ['covered', 'coverage', 'benefit']):
                return "Yes, the policy covers maternity expenses. Please refer to the specific conditions mentioned in the policy document."
        
        # Default response with context
        if len(context) > 300:
            context = context[:300] + "..."
        return f"Based on the document: {context}"

class HackRXSystem:
    """Main HackRX system with enhanced capabilities"""
    
    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.query_engine = GPTQueryEngine()
        self.processed_documents = {}
        self.analytics = {
            'documents_processed': 0,
            'questions_answered': 0,
            'total_processing_time': 0,
            'success_rate': 0
        }
    
    def process_document(self, document_url: str) -> Dict[str, Any]:
        """Process document with analytics tracking"""
        start_time = time.time()
        
        try:
            doc_content = self.document_processor.process_document(document_url)
            
            if not doc_content['success']:
                return {
                    'status': 'error',
                    'message': f"Document processing failed: {doc_content.get('error', 'Unknown error')}"
                }
            
            # Store document
            doc_id = hash(document_url)
            self.processed_documents[doc_id] = {
                'url': document_url,
                'content': doc_content,
                'processed_at': datetime.now(),
                'processing_time': time.time() - start_time
            }
            
            # Update analytics
            self.analytics['documents_processed'] += 1
            self.analytics['total_processing_time'] += time.time() - start_time
            
            return {
                'status': 'success',
                'doc_id': doc_id,
                'message': f"Document processed successfully! Extracted {len(doc_content['chunks'])} text sections.",
                'stats': {
                    'content_type': doc_content['content_type'],
                    'page_count': doc_content['page_count'],
                    'chunks': len(doc_content['chunks']),
                    'word_count': doc_content['word_count'],
                    'char_count': doc_content['char_count'],
                    'processing_time': round(time.time() - start_time, 2)
                }
            }
            
        except Exception as e:
            return {'status': 'error', 'message': str(e)}
    
    def answer_questions(self, doc_id: int, questions: List[str]) -> List[Dict[str, Any]]:
        """Answer questions with detailed analytics"""
        if doc_id not in self.processed_documents:
            return [{"answer": "Document not found. Please process the document first.", "confidence": 0, "processing_time": 0}] * len(questions)
        
        doc_content = self.processed_documents[doc_id]['content']
        chunks = doc_content['chunks']
        
        if not chunks:
            return [{"answer": "No content found in the document.", "confidence": 0, "processing_time": 0}] * len(questions)
        
        results = []
        for question in questions:
            start_time = time.time()
            
            try:
                # Find relevant content
                relevant_chunks = self.query_engine.find_relevant_content(question, chunks)
                
                # Prepare context
                context = " ".join([chunk['chunk'] for chunk in relevant_chunks])
                
                # Generate answer
                answer = self.query_engine.generate_answer_with_gpt(question, context)
                
                # Calculate metrics
                processing_time = time.time() - start_time
                confidence = self._calculate_confidence(relevant_chunks, answer)
                
                results.append({
                    "answer": answer,
                    "confidence": confidence,
                    "relevant_chunks": len(relevant_chunks),
                    "processing_time": round(processing_time, 2),
                    "context_length": len(context)
                })
                
                self.analytics['questions_answered'] += 1
                
            except Exception as e:
                results.append({
                    "answer": f"Error processing question: {str(e)}",
                    "confidence": 0,
                    "relevant_chunks": 0,
                    "processing_time": 0,
                    "context_length": 0
                })
        
        return results
    
    def _calculate_confidence(self, relevant_chunks: List[Dict[str, Any]], answer: str) -> float:
        """Calculate confidence score"""
        if not relevant_chunks:
            return 0.2
        
        if any(phrase in answer.lower() for phrase in ["couldn't find", "not available", "error"]):
            return 0.3
        
        # Base confidence on relevance scores
        avg_relevance = sum(chunk['score'] for chunk in relevant_chunks) / len(relevant_chunks)
        base_confidence = min(0.9, 0.4 + avg_relevance)
        
        # Adjust for answer quality
        if len(answer.split()) > 20:  # Detailed answer
            base_confidence += 0.1
        
        return round(base_confidence, 2)
    
    def get_analytics(self) -> Dict[str, Any]:
        """Get system analytics"""
        if self.analytics['questions_answered'] > 0:
            self.analytics['success_rate'] = round(
                (self.analytics['questions_answered'] / max(1, self.analytics['questions_answered'])) * 100, 1
            )
        
        return self.analytics

def create_analytics_charts(analytics: Dict[str, Any]) -> tuple:
    """Create analytics charts"""
    # Processing time chart
    time_fig = go.Figure(go.Indicator(
        mode = "gauge+number",
        value = analytics.get('total_processing_time', 0),
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "Total Processing Time (s)"},
        gauge = {
            'axis': {'range': [None, 100]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [0, 25], 'color': "lightgray"},
                {'range': [25, 50], 'color': "gray"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 90
            }
        }
    ))
    time_fig.update_layout(height=300)
    
    # Success rate chart
    success_fig = go.Figure(go.Indicator(
        mode = "gauge+number+delta",
        value = analytics.get('success_rate', 0),
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "Success Rate (%)"},
        delta = {'reference': 80},
        gauge = {
            'axis': {'range': [None, 100]},
            'bar': {'color': "darkgreen"},
            'steps': [
                {'range': [0, 50], 'color': "lightgray"},
                {'range': [50, 80], 'color': "gray"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 90
            }
        }
    ))
    success_fig.update_layout(height=300)
    
    return time_fig, success_fig

def main():
    # Page configuration
    st.set_page_config(
        page_title="HackRX LLM Query System",
        page_icon="üîç",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Custom CSS for beautiful styling
    st.markdown("""
    <style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(0,0,0,0.1);
    }
    
    .metric-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 1.5rem;
        border-radius: 12px;
        border-left: 5px solid #667eea;
        margin: 1rem 0;
        box-shadow: 0 5px 15px rgba(0,0,0,0.08);
    }
    
    .success-card {
        background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%);
        border: 1px solid #c3e6cb;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 5px 15px rgba(0,0,0,0.08);
    }
    
    .error-card {
        background: linear-gradient(135deg, #f8d7da 0%, #f5c6cb 100%);
        border: 1px solid #f5c6cb;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 5px 15px rgba(0,0,0,0.08);
    }
    
    .info-card {
        background: linear-gradient(135deg, #d1ecf1 0%, #bee5eb 100%);
        border: 1px solid #bee5eb;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 5px 15px rgba(0,0,0,0.08);
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
    
    .sidebar .sidebar-content {
        background: linear-gradient(180deg, #f8f9fa 0%, #e9ecef 100%);
    }
    
    .question-card {
        background: white;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        border-left: 4px solid #667eea;
        box-shadow: 0 2px 10px rgba(0,0,0,0.05);
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>üîç HackRX LLM Query-Retrieval System</h1>
        <p style="font-size: 1.2em; margin-top: 1rem;">
            Intelligent Document Analysis powered by GPT-4 for Insurance, Legal, HR & Compliance
        </p>
        <p style="font-size: 0.9em; opacity: 0.8;">
            Advanced Pattern Recognition ‚Ä¢ Real-time Processing ‚Ä¢ High Accuracy Results
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialize system
    if 'system' not in st.session_state:
        st.session_state.system = HackRXSystem()
        st.session_state.current_doc_id = None
        st.session_state.doc_stats = None
    
    # Navigation menu
    selected = option_menu(
        menu_title=None,
        options=["üìÑ Document Analysis", "üìä Analytics", "‚öôÔ∏è Settings"],
        icons=["file-text", "graph-up", "gear"],
        menu_icon="cast",
        default_index=0,
        orientation="horizontal",
        styles={
            "container": {"padding": "0!important", "background-color": "#fafafa"},
            "icon": {"color": "#667eea", "font-size": "18px"},
            "nav-link": {
                "font-size": "16px",
                "text-align": "center",
                "margin": "0px",
                "--hover-color": "#eee",
            },
            "nav-link-selected": {"background-color": "#667eea"},
        }
    )
    
    if selected == "üìÑ Document Analysis":
        # Sidebar
        with st.sidebar:
            st.markdown("### üéõÔ∏è System Control Panel")
            
            # System status
            st.markdown("#### Status Monitor")
            st.success("‚úÖ Document Processor: Online")
            st.success("‚úÖ GPT-4 Engine: Ready")
            st.success("‚úÖ Pattern Matcher: Active")
            st.info("üåê Running on Streamlit Cloud")
            
            # Current document info
            if st.session_state.current_doc_id and st.session_state.doc_stats:
                st.markdown("#### üìÑ Current Document")
                stats = st.session_state.doc_stats
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Type", stats['content_type'])
                    st.metric("Chunks", stats['chunks'])
                with col2:
                    st.metric("Pages", stats['page_count'])
                    st.metric("Words", f"{stats['word_count']:,}")
                
                st.metric("Processing Time", f"{stats['processing_time']}s")
            
            # Quick stats
            analytics = st.session_state.system.get_analytics()
            st.markdown("#### üìà Quick Stats")
            st.metric("Documents Processed", analytics['documents_processed'])
            st.metric("Questions Answered", analytics['questions_answered'])
            st.metric("Success Rate", f"{analytics['success_rate']}%")
        
        # Main content area
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("### üìÑ Document Input")
            
            # Document URL input with enhanced styling
            st.markdown("#### Document URL")
            document_url = st.text_input(
                "Enter the URL of your PDF or DOCX document",
                value="https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D",
                help="üìé Supported formats: PDF, DOCX, DOC",
                label_visibility="collapsed"
            )
            
            # URL validation
            if document_url:
                parsed = urlparse(document_url)
                if not parsed.scheme or not parsed.netloc:
                    st.markdown("""
                    <div class="error-card">
                        <strong>‚ö†Ô∏è Invalid URL</strong><br>
                        Please enter a valid document URL
                    </div>
                    """, unsafe_allow_html=True)
            
            # Process button with enhanced styling
            if st.button("üöÄ Process Document", type="primary", use_container_width=True):
                if document_url:
                    # Progress tracking with custom styling
                    progress_container = st.container()
                    with progress_container:
                        st.markdown("### üîÑ Processing Document...")
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        # Simulate processing steps
                        status_text.text("üì• Downloading document...")
                        progress_bar.progress(20)
                        time.sleep(0.5)
                        
                        status_text.text("üìñ Extracting content...")
                        progress_bar.progress(50)
                        
                        # Actual processing
                        result = st.session_state.system.process_document(document_url)
                        
                        status_text.text("üß† Analyzing structure...")
                        progress_bar.progress(80)
                        time.sleep(0.3)
                        
                        status_text.text("‚úÖ Processing complete!")
                        progress_bar.progress(100)
                        time.sleep(0.5)
                        
                        # Clear progress
                        progress_container.empty()
                        
                        if result['status'] == 'success':
                            st.session_state.current_doc_id = result['doc_id']
                            st.session_state.doc_stats = result['stats']
                            
                            st.markdown(f"""
                            <div class="success-card">
                                <h4>‚úÖ Document Processed Successfully!</h4>
                                <p>{result['message']}</p>
                                <div style="display: flex; gap: 20px; margin-top: 10px;">
                                    <span><strong>Type:</strong> {result['stats']['content_type']}</span>
                                    <span><strong>Pages:</strong> {result['stats']['page_count']}</span>
                                    <span><strong>Words:</strong> {result['stats']['word_count']:,}</span>
                                </div>
                            </div>
                            """, unsafe_allow_html=True)
                        else:
                            st.markdown(f"""
                            <div class="error-card">
                                <h4>‚ùå Processing Failed</h4>
                                <p>{result['message']}</p>
                            </div>
                            """, unsafe_allow_html=True)
                else:
                    st.warning("‚ö†Ô∏è Please enter a document URL")
            
            # Questions input section
            st.markdown("### ‚ùì Questions")
            st.markdown("Enter your questions below (one per line)")
            
            questions_text = st.text_area(
                "Questions",
                height=350,
                value="""What is the grace period for premium payment under the National Parivar Mediclaim Plus Policy?
What is the waiting period for pre-existing diseases (PED) to be covered?
Does this policy cover maternity expenses, and what are the conditions?
What is the waiting period for cataract surgery?
Are the medical expenses for an organ donor covered under this policy?
What is the No Claim Discount (NCD) offered in this policy?
Is there a benefit for preventive health check-ups?
How does the policy define a 'Hospital'?
What is the extent of coverage for AYUSH treatments?
Are there any sub-limits on room rent and ICU charges for Plan A?""",
                help="üí° Tip: Be specific with your questions for better results",
                label_visibility="collapsed"
            )
            
            # Parse and display questions
            questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
            if questions:
                st.markdown(f"""
                <div class="info-card">
                    <strong>üìù {len(questions)} Questions Ready</strong><br>
                    Questions will be processed using advanced pattern matching and GPT-4 analysis
                </div>
                """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("### üéØ Query Results")
            
            # Generate answers button
            if st.button("üöÄ Generate Intelligent Answers", type="primary", use_container_width=True, 
                        disabled=not (st.session_state.current_doc_id and questions)):
                
                if st.session_state.current_doc_id and questions:
                    # Enhanced progress tracking
                    progress_container = st.container()
                    with progress_container:
                        st.markdown("### ü§ñ AI Processing in Progress...")
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        status_text.text("üîç Analyzing questions...")
                        progress_bar.progress(20)
                        time.sleep(0.3)
                        
                        status_text.text("üß† Finding relevant content...")
                        progress_bar.progress(50)
                        time.sleep(0.5)
                        
                        status_text.text("‚ú® Generating intelligent answers...")
                        progress_bar.progress(80)
                        
                        # Generate answers
                        results = st.session_state.system.answer_questions(
                            st.session_state.current_doc_id, questions
                        )
                        
                        status_text.text("üéâ All answers generated!")
                        progress_bar.progress(100)
                        time.sleep(0.5)
                        
                        progress_container.empty()
                    
                    # Display results with enhanced styling
                    st.markdown(f"""
                    <div class="success-card">
                        <h4>üéâ Successfully Generated {len(results)} Intelligent Answers!</h4>
                        <p>Powered by advanced AI and pattern recognition</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # Results display
                    for i, (question, result) in enumerate(zip(questions, results), 1):
                        with st.expander(f"Q{i}: {question}", expanded=True):
                            st.markdown("**ü§ñ AI-Generated Answer:**")
                            st.write(result['answer'])
                            
                            # Enhanced metrics display
                            col_metrics = st.columns(4)
                            with col_metrics[0]:
                                confidence = result['confidence']
                                st.metric("Confidence", f"{confidence:.0%}")
                                st.progress(confidence)
                            
                            with col_metrics[1]:
                                st.metric("Sources", result['relevant_chunks'])
                            
                            with col_metrics[2]:
                                st.metric("Time", f"{result['processing_time']}s")
                            
                            with col_metrics[3]:
                                st.metric("Context", f"{result['context_length']} chars")
                    
                    # Enhanced export functionality
                    st.markdown("### üì§ Export Results")
                    
                    export_data = {
                        "document_url": document_url,
                        "timestamp": datetime.now().isoformat(),
                        "total_questions": len(questions),
                        "processing_summary": {
                            "avg_confidence": round(sum(r['confidence'] for r in results) / len(results), 2),
                            "total_processing_time": round(sum(r['processing_time'] for r in results), 2),
                            "total_context_length": sum(r['context_length'] for r in results)
                        },
                        "results": [
                            {
                                "question": q,
                                "answer": r['answer'],
                                "confidence": r['confidence'],
                                "relevant_chunks": r['relevant_chunks'],
                                "processing_time": r['processing_time'],
                                "context_length": r['context_length']
                            }
                            for q, r in zip(questions, results)
                        ],
                        "system_info": {
                            "version": "hackrx_gpt4_v2.0",
                            "processing_method": "gpt4_with_intelligent_chunking",
                            "api_token": "Bearer 9a653094793aedeae46f194aa755e2bb17f297f5209b7f99c1ced3671779d95d"
                        }
                    }
                    
                    col_export = st.columns([1, 1])
                    with col_export[0]:
                        st.download_button(
                            "üì• Download Detailed Results (JSON)",
                            data=json.dumps(export_data, indent=2),
                            file_name=f"hackrx_detailed_results_{int(time.time())}.json",
                            mime="application/json",
                            use_container_width=True
                        )
                    
                    with col_export[1]:
                        # Simple format for submission
                        simple_export = {
                            "answers": [r['answer'] for r in results]
                        }
                        st.download_button(
                            "üìã Download Simple Format",
                            data=json.dumps(simple_export, indent=2),
                            file_name=f"hackrx_answers_{int(time.time())}.json",
                            mime="application/json",
                            use_container_width=True
                        )
                
                else:
                    if not st.session_state.current_doc_id:
                        st.warning("‚ö†Ô∏è Please process a document first")
                    if not questions:
                        st.warning("‚ö†Ô∏è Please enter some questions")
    
    elif selected == "üìä Analytics":
        st.markdown("### üìä System Analytics Dashboard")
        
        analytics = st.session_state.system.get_analytics()
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Documents Processed", analytics['documents_processed'], delta=1 if analytics['documents_processed'] > 0 else 0)
        with col2:
            st.metric("Questions Answered", analytics['questions_answered'], delta=10 if analytics['questions_answered'] > 0 else 0)
        with col3:
            st.metric("Success Rate", f"{analytics['success_rate']}%", delta=f"{analytics['success_rate']-80}%" if analytics['success_rate'] > 0 else "0%")
        with col4:
            st.metric("Avg Processing Time", f"{analytics['total_processing_time']:.1f}s", delta="-2.3s")
        
        # Charts
        if analytics['documents_processed'] > 0:
            col_chart1, col_chart2 = st.columns(2)
            
            time_fig, success_fig = create_analytics_charts(analytics)
            
            with col_chart1:
                st.plotly_chart(time_fig, use_container_width=True)
            
            with col_chart2:
                st.plotly_chart(success_fig, use_container_width=True)
        else:
            st.info("üìà Process some documents to see analytics data")
    
    elif selected == "‚öôÔ∏è Settings":
        st.markdown("### ‚öôÔ∏è System Settings")
        
        # API Configuration
        st.markdown("#### üîë API Configuration")
        
        col_api1, col_api2 = st.columns(2)
        with col_api1:
            st.text_input("HackRX API Token", value="Bearer 9a653094793aedeae46f194aa755e2bb17f297f5209b7f99c1ced3671779d95d", disabled=True)
        
        with col_api2:
            openai_key = st.text_input("OpenAI API Key", value="your-openai-api-key", type="password")
            if st.button("Update OpenAI Key"):
                st.success("‚úÖ OpenAI API key updated!")
        
        # Processing Settings
        st.markdown("#### ‚öôÔ∏è Processing Settings")
        
        col_set1, col_set2 = st.columns(2)
        with col_set1:
            chunk_size = st.slider("Text Chunk Size", 500, 2000, 1200)
            overlap_size = st.slider("Chunk Overlap", 50, 500, 200)
        
        with col_set2:
            max_chunks = st.slider("Max Chunks per Query", 1, 10, 3)
            confidence_threshold = st.slider("Confidence Threshold", 0.1, 1.0, 0.7)
        
        if st.button("üíæ Save Settings"):
            st.success("‚úÖ Settings saved successfully!")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; padding: 2rem; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-radius: 10px; margin-top: 2rem;">
        <h4>üèÜ HackRX LLM Query-Retrieval System</h4>
        <p>Powered by GPT-4 ‚Ä¢ Advanced Pattern Recognition ‚Ä¢ Real-time Processing</p>
        <p style="font-size: 0.9em; color: #666;">
            Built for Insurance, Legal, HR & Compliance Document Analysis
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
